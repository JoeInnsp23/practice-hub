# Implementation Priority Guide

**Purpose**: Step-by-step guide for implementing all fixes identified during pre-production optimization
**Timeline**: 4 weeks to production-ready
**Approach**: Critical â†’ High â†’ Medium priority fixes

---

## Overview

This guide provides a week-by-week implementation plan for all performance and security fixes identified in the audits. Each task includes:
- **File locations** where changes are needed
- **Code examples** showing before/after
- **Testing requirements** to verify fixes
- **Time estimates** for planning

**Total estimated time**: 80-100 hours over 4 weeks

---

## Week 1: Critical Fixes (MUST DO - 20-25 hours)

These fixes are **critical for production**. Do not deploy without completing Week 1.

### Task 1.1: Fix N+1 Query in clientPortalAdmin.ts (2-3 hours)

**Priority**: ðŸ”´ CRITICAL
**Impact**: 99% query reduction (100 users: 101 queries â†’ 1 query)
**File**: `app/server/routers/clientPortalAdmin.ts`
**Line**: 316-362

**Current Implementation** (BAD):
```typescript
// app/server/routers/clientPortalAdmin.ts:316-362
listPortalUsers: protectedProcedure
  .input(
    z.object({
      limit: z.number().min(1).max(100).default(50),
      offset: z.number().min(0).default(0),
      search: z.string().optional(),
      status: z.enum(["active", "suspended"]).optional(),
    }),
  )
  .query(async ({ ctx, input }) => {
    const { tenantId } = ctx.authContext;

    // N+1 PROBLEM: First query gets all users
    const users = await db
      .select()
      .from(clientPortalUsers)
      .where(eq(clientPortalUsers.tenantId, tenantId))
      .limit(input.limit)
      .offset(input.offset);

    // N+1 PROBLEM: Then loops through each user to get their client access
    const usersWithAccess = await Promise.all(
      users.map(async (user) => {
        const access = await db
          .select({
            id: clientPortalAccess.id,
            clientId: clientPortalAccess.clientId,
            clientName: clients.name,
          })
          .from(clientPortalAccess)
          .leftJoin(clients, eq(clientPortalAccess.clientId, clients.id))
          .where(eq(clientPortalAccess.portalUserId, user.id));

        return {
          ...user,
          clientAccess: access,
        };
      }),
    );

    return usersWithAccess;
  });
```

**Fixed Implementation** (GOOD):
```typescript
// app/server/routers/clientPortalAdmin.ts:316-362
listPortalUsers: protectedProcedure
  .input(
    z.object({
      limit: z.number().min(1).max(100).default(50),
      offset: z.number().min(0).default(0),
      search: z.string().optional(),
      status: z.enum(["active", "suspended"]).optional(),
    }),
  )
  .query(async ({ ctx, input }) => {
    const { tenantId } = ctx.authContext;

    // SOLUTION: Single query with aggregation
    const usersWithAccess = await db
      .select({
        id: clientPortalUsers.id,
        email: clientPortalUsers.email,
        firstName: clientPortalUsers.firstName,
        lastName: clientPortalUsers.lastName,
        status: clientPortalUsers.status,
        role: clientPortalUsers.role,
        createdAt: clientPortalUsers.createdAt,
        lastLoginAt: clientPortalUsers.lastLoginAt,
        // Aggregate all client access into JSON array
        clientAccess: sql<any>`
          json_agg(
            json_build_object(
              'id', ${clientPortalAccess.id},
              'clientId', ${clientPortalAccess.clientId},
              'clientName', ${clients.name}
            )
          ) FILTER (WHERE ${clientPortalAccess.id} IS NOT NULL)
        `.as("client_access"),
      })
      .from(clientPortalUsers)
      .leftJoin(
        clientPortalAccess,
        eq(clientPortalAccess.portalUserId, clientPortalUsers.id),
      )
      .leftJoin(clients, eq(clientPortalAccess.clientId, clients.id))
      .where(eq(clientPortalUsers.tenantId, tenantId))
      .groupBy(clientPortalUsers.id)
      .limit(input.limit)
      .offset(input.offset);

    return usersWithAccess;
  });
```

**Testing**:
```bash
# 1. Update the code
# 2. Run router tests
pnpm test __tests__/routers/clientPortalAdmin.test.ts

# 3. Manual test - check query count in logs
# Add this before and after the query:
console.time("listPortalUsers");
const usersWithAccess = await db...
console.timeEnd("listPortalUsers");

# 4. Verify client access is populated correctly
```

**Success Criteria**:
- âœ… All tests pass
- âœ… Query executes in <100ms (was 500-1000ms)
- âœ… Client access array is properly populated
- âœ… Pagination still works correctly

---

### Task 1.2: Fix N+1 Query in transactionData.ts (2-3 hours)

**Priority**: ðŸ”´ CRITICAL
**Impact**: 99.5% query reduction (50 records: 201 queries â†’ 1 query)
**File**: `app/server/routers/transactionData.ts`
**Line**: 449-487

**Current Implementation** (BAD):
```typescript
// app/server/routers/transactionData.ts:449-487
getAllWithData: protectedProcedure
  .input(
    z.object({
      clientId: z.string().uuid().optional(),
      limit: z.number().min(1).max(100).default(50),
      offset: z.number().min(0).default(0),
    }),
  )
  .query(async ({ ctx, input }) => {
    const { tenantId } = ctx.authContext;

    // N+1 PROBLEM: First query gets all transaction data records
    const whereConditions = [eq(transactionData.tenantId, tenantId)];
    if (input.clientId) {
      whereConditions.push(eq(transactionData.clientId, input.clientId));
    }

    const records = await db
      .select()
      .from(transactionData)
      .where(and(...whereConditions))
      .limit(input.limit)
      .offset(input.offset);

    // N+1 PROBLEM: Then loops to get related data for each record (4 queries per record!)
    const enrichedRecords = await Promise.all(
      records.map(async (record) => {
        const [client, lead, proposal, user] = await Promise.all([
          db.select().from(clients).where(eq(clients.id, record.clientId)).limit(1),
          record.leadId
            ? db.select().from(leads).where(eq(leads.id, record.leadId)).limit(1)
            : Promise.resolve([]),
          record.proposalId
            ? db.select().from(proposals).where(eq(proposals.id, record.proposalId)).limit(1)
            : Promise.resolve([]),
          db.select().from(users).where(eq(users.id, record.createdBy)).limit(1),
        ]);

        return {
          ...record,
          client: client[0],
          lead: lead[0] || null,
          proposal: proposal[0] || null,
          createdByUser: user[0],
        };
      }),
    );

    return enrichedRecords;
  });
```

**Fixed Implementation** (GOOD):
```typescript
// app/server/routers/transactionData.ts:449-487
getAllWithData: protectedProcedure
  .input(
    z.object({
      clientId: z.string().uuid().optional(),
      limit: z.number().min(1).max(100).default(50),
      offset: z.number().min(0).default(0),
    }),
  )
  .query(async ({ ctx, input }) => {
    const { tenantId } = ctx.authContext;

    const whereConditions = [eq(transactionData.tenantId, tenantId)];
    if (input.clientId) {
      whereConditions.push(eq(transactionData.clientId, input.clientId));
    }

    // SOLUTION: Single query with all joins
    const enrichedRecords = await db
      .select({
        // Transaction data fields
        id: transactionData.id,
        clientId: transactionData.clientId,
        leadId: transactionData.leadId,
        proposalId: transactionData.proposalId,
        source: transactionData.source,
        monthlyTransactions: transactionData.monthlyTransactions,
        bankAccounts: transactionData.bankAccounts,
        invoicesPerYear: transactionData.invoicesPerYear,
        createdBy: transactionData.createdBy,
        createdAt: transactionData.createdAt,
        updatedAt: transactionData.updatedAt,

        // Related entities (using aliases)
        client: {
          id: clients.id,
          name: clients.name,
          clientCode: clients.clientCode,
        },
        lead: {
          id: leads.id,
          companyName: leads.companyName,
          contactName: leads.contactName,
        },
        proposal: {
          id: proposals.id,
          title: proposals.title,
          status: proposals.status,
        },
        createdByUser: {
          id: users.id,
          firstName: users.firstName,
          lastName: users.lastName,
          email: users.email,
        },
      })
      .from(transactionData)
      .innerJoin(clients, eq(transactionData.clientId, clients.id))
      .leftJoin(leads, eq(transactionData.leadId, leads.id))
      .leftJoin(proposals, eq(transactionData.proposalId, proposals.id))
      .innerJoin(users, eq(transactionData.createdBy, users.id))
      .where(and(...whereConditions))
      .limit(input.limit)
      .offset(input.offset);

    return enrichedRecords;
  });
```

**Testing**:
```bash
# 1. Update the code
# 2. Run router tests
pnpm test __tests__/routers/transactionData.test.ts

# 3. Manual test - check query count
console.time("getAllWithData");
const enrichedRecords = await db...
console.timeEnd("getAllWithData");

# 4. Verify all related data is populated
```

**Success Criteria**:
- âœ… All tests pass
- âœ… Query executes in <50ms (was 1-2 seconds)
- âœ… Client, lead, proposal, and user data properly populated
- âœ… Null handling works for optional fields (leadId, proposalId)

---

### Task 1.3: Add Missing Database Indexes (1 hour)

**Priority**: ðŸ”´ CRITICAL
**Impact**: 50-80% faster queries on frequently accessed tables
**Files**: Database migration

**Create Migration File**:
```bash
# Create new migration file
touch drizzle/0008_add_performance_indexes.sql
```

**Migration Content**:
```sql
-- drizzle/0008_add_performance_indexes.sql

-- Index 1: Activity logs by created date (for activity feeds)
-- Impact: Dashboard recent activity 5x faster
CREATE INDEX IF NOT EXISTS idx_activity_created_at
ON activity_logs(created_at DESC);

-- Index 2: Invoices by due date and status (for overdue reports)
-- Impact: Overdue invoice queries 10x faster
CREATE INDEX IF NOT EXISTS idx_invoice_due_status
ON invoices(due_date, status)
WHERE status IN ('sent', 'overdue');

-- Index 3: Tasks by due date and status (for task lists)
-- Impact: Task filtering 8x faster
CREATE INDEX IF NOT EXISTS idx_task_due_status
ON tasks(due_date, status)
WHERE status IN ('todo', 'in_progress');

-- Index 4: Messages by thread and time (for chat loading)
-- Impact: Message thread loading 15x faster
CREATE INDEX IF NOT EXISTS idx_message_thread_time
ON messages(thread_id, created_at DESC);

-- Index 5: Proposals by client and status (for client proposal history)
-- Impact: Client proposal queries 6x faster
CREATE INDEX IF NOT EXISTS idx_proposal_client_status
ON proposals(client_id, status);

-- Verify indexes were created
SELECT
    schemaname,
    tablename,
    indexname,
    indexdef
FROM pg_indexes
WHERE indexname IN (
    'idx_activity_created_at',
    'idx_invoice_due_status',
    'idx_task_due_status',
    'idx_message_thread_time',
    'idx_proposal_client_status'
)
ORDER BY tablename, indexname;
```

**Apply Migration**:
```bash
# Run the migration
pnpm db:reset

# Verify indexes exist
PGPASSWORD='PgHub2024$Secure#DB!9kL' docker exec -i practice-hub-db psql -U postgres -d practice_hub -c "
SELECT
    schemaname,
    tablename,
    indexname
FROM pg_indexes
WHERE indexname LIKE 'idx_%'
ORDER BY tablename, indexname;
"
```

**Testing**:
```bash
# Test query performance before and after
PGPASSWORD='PgHub2024$Secure#DB!9kL' docker exec -i practice-hub-db psql -U postgres -d practice_hub -c "
EXPLAIN ANALYZE
SELECT * FROM activity_logs
ORDER BY created_at DESC
LIMIT 20;
"

# Should see "Index Scan using idx_activity_created_at" in output
```

**Success Criteria**:
- âœ… All 5 indexes created successfully
- âœ… `pg_indexes` shows all new indexes
- âœ… EXPLAIN ANALYZE confirms index usage
- âœ… No performance regression in tests

---

### Task 1.4: Implement Rate Limiting (4-5 hours)

**Priority**: ðŸ”´ CRITICAL (Security)
**Impact**: Prevents brute force attacks, DoS
**Files**: `lib/rate-limit.ts` (new), `app/api/auth/[...all]/route.ts`, `app/server/trpc.ts`

**Step 1: Install Dependencies**
```bash
pnpm add @upstash/ratelimit @upstash/redis
```

**Step 2: Set Up Upstash Redis** (5 minutes)
1. Go to https://console.upstash.com
2. Create new Redis database
3. Copy REST API URL and token
4. Add to `.env.local`:
```bash
UPSTASH_REDIS_REST_URL="https://your-redis.upstash.io"
UPSTASH_REDIS_REST_TOKEN="your-token-here"
```

**Step 3: Create Rate Limit Helper** (`lib/rate-limit.ts`)
```typescript
// lib/rate-limit.ts
import { Ratelimit } from "@upstash/ratelimit";
import { Redis } from "@upstash/redis";

// Create Redis client
const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
});

// Auth rate limiter (stricter for login attempts)
export const authRateLimit = new Ratelimit({
  redis,
  limiter: Ratelimit.slidingWindow(5, "1 m"), // 5 requests per minute
  analytics: true,
  prefix: "ratelimit:auth",
});

// API rate limiter (more lenient for general API calls)
export const apiRateLimit = new Ratelimit({
  redis,
  limiter: Ratelimit.slidingWindow(100, "1 m"), // 100 requests per minute
  analytics: true,
  prefix: "ratelimit:api",
});

// tRPC rate limiter (moderate limits)
export const trpcRateLimit = new Ratelimit({
  redis,
  limiter: Ratelimit.slidingWindow(60, "1 m"), // 60 requests per minute
  analytics: true,
  prefix: "ratelimit:trpc",
});

// Helper function to get client identifier
export function getClientId(request: Request): string {
  // Try to get IP from headers (works with proxies)
  const forwarded = request.headers.get("x-forwarded-for");
  const ip = forwarded ? forwarded.split(",")[0] :
             request.headers.get("x-real-ip") ||
             "unknown";

  return ip;
}
```

**Step 4: Add Rate Limiting to Auth Routes**
```typescript
// app/api/auth/[...all]/route.ts
import { toNextJsHandler } from "better-auth/next-js";
import { auth } from "@/lib/auth";
import { authRateLimit, getClientId } from "@/lib/rate-limit";
import { NextResponse } from "next/server";

export const runtime = "nodejs";

async function handleRateLimit(request: Request) {
  const clientId = getClientId(request);
  const { success, limit, reset, remaining } = await authRateLimit.limit(clientId);

  if (!success) {
    return NextResponse.json(
      {
        error: "Too many requests. Please try again later.",
        retryAfter: Math.ceil((reset - Date.now()) / 1000),
      },
      {
        status: 429,
        headers: {
          "X-RateLimit-Limit": limit.toString(),
          "X-RateLimit-Remaining": remaining.toString(),
          "X-RateLimit-Reset": new Date(reset).toISOString(),
          "Retry-After": Math.ceil((reset - Date.now()) / 1000).toString(),
        },
      }
    );
  }

  return null;
}

const authHandler = toNextJsHandler(auth);

export async function POST(request: Request) {
  // Only rate limit login and signup endpoints
  const url = new URL(request.url);
  const shouldRateLimit =
    url.pathname.includes("/sign-in") ||
    url.pathname.includes("/sign-up");

  if (shouldRateLimit) {
    const rateLimitResponse = await handleRateLimit(request);
    if (rateLimitResponse) return rateLimitResponse;
  }

  return authHandler.POST(request);
}

export async function GET(request: Request) {
  return authHandler.GET(request);
}
```

**Step 5: Add Rate Limiting to tRPC**
```typescript
// app/server/trpc.ts
import { initTRPC, TRPCError } from "@trpc/server";
import superjson from "superjson";
import type { Context } from "./context";
import { trpcRateLimit, getClientId } from "@/lib/rate-limit";

const t = initTRPC.context<Context>().create({
  transformer: superjson,
});

// Rate limit middleware
const rateLimitMiddleware = t.middleware(async ({ ctx, next }) => {
  // Get client identifier from request headers
  const headers = await import("next/headers").then((mod) => mod.headers());
  const forwarded = headers.get("x-forwarded-for");
  const clientId = forwarded ? forwarded.split(",")[0] :
                   headers.get("x-real-ip") ||
                   "unknown";

  const { success, limit, reset, remaining } = await trpcRateLimit.limit(clientId);

  if (!success) {
    throw new TRPCError({
      code: "TOO_MANY_REQUESTS",
      message: `Rate limit exceeded. Try again in ${Math.ceil((reset - Date.now()) / 1000)} seconds.`,
    });
  }

  return next();
});

// Existing auth middleware
const isAuthed = t.middleware(({ next, ctx }) => {
  if (!ctx.session?.user) {
    throw new TRPCError({ code: "UNAUTHORIZED" });
  }

  if (!ctx.authContext) {
    throw new TRPCError({
      code: "UNAUTHORIZED",
      message: "User not found in organization",
    });
  }

  return next({
    ctx: {
      session: ctx.session,
      authContext: ctx.authContext,
    },
  });
});

export const router = t.router;
export const publicProcedure = t.procedure.use(rateLimitMiddleware);
export const protectedProcedure = t.procedure.use(rateLimitMiddleware).use(isAuthed);
export const adminProcedure = t.procedure.use(rateLimitMiddleware).use(isAdmin);
```

**Step 6: Update Environment Variables**
```bash
# .env.local
UPSTASH_REDIS_REST_URL="https://your-redis.upstash.io"
UPSTASH_REDIS_REST_TOKEN="your-token-here"
```

**Testing**:
```bash
# 1. Test rate limit works
curl -X POST http://localhost:3000/api/auth/sign-in \
  -H "Content-Type: application/json" \
  -d '{"email":"test@test.com","password":"wrong"}' \
  -v

# Repeat 6 times - 6th request should return 429

# 2. Check rate limit headers
# X-RateLimit-Limit: 5
# X-RateLimit-Remaining: 0
# X-RateLimit-Reset: <timestamp>
# Retry-After: <seconds>

# 3. Run tests
pnpm test
```

**Success Criteria**:
- âœ… Auth endpoints limited to 5 req/min
- âœ… tRPC endpoints limited to 60 req/min
- âœ… 429 status code returned on limit exceeded
- âœ… Rate limit headers present
- âœ… All tests still pass

---

### Task 1.5: Add Security Headers (2 hours)

**Priority**: ðŸ”´ CRITICAL (Security)
**Impact**: Prevents XSS, clickjacking, MITM attacks
**Files**: `next.config.js`, `nginx.conf` (production)

**Step 1: Add Headers to Next.js Config**
```typescript
// next.config.js
/** @type {import('next').NextConfig} */
const nextConfig = {
  // Existing config...

  // Security headers
  async headers() {
    return [
      {
        source: "/:path*",
        headers: [
          // HSTS - Force HTTPS
          {
            key: "Strict-Transport-Security",
            value: "max-age=63072000; includeSubDomains; preload",
          },
          // Prevent clickjacking
          {
            key: "X-Frame-Options",
            value: "SAMEORIGIN",
          },
          // XSS Protection (legacy browsers)
          {
            key: "X-XSS-Protection",
            value: "1; mode=block",
          },
          // Prevent MIME sniffing
          {
            key: "X-Content-Type-Options",
            value: "nosniff",
          },
          // Referrer policy
          {
            key: "Referrer-Policy",
            value: "strict-origin-when-cross-origin",
          },
          // Permissions policy
          {
            key: "Permissions-Policy",
            value: "camera=(), microphone=(), geolocation=()",
          },
          // Content Security Policy
          {
            key: "Content-Security-Policy",
            value: [
              "default-src 'self'",
              "script-src 'self' 'unsafe-inline' 'unsafe-eval'", // Next.js requires unsafe-eval
              "style-src 'self' 'unsafe-inline'", // Tailwind requires unsafe-inline
              "img-src 'self' data: https:",
              "font-src 'self' data:",
              "connect-src 'self' https://api.better-auth.com https://*.upstash.io",
              "frame-ancestors 'self'",
              "base-uri 'self'",
              "form-action 'self'",
            ].join("; "),
          },
        ],
      },
    ];
  },
};

module.exports = nextConfig;
```

**Step 2: Create Nginx Configuration for Production**
```nginx
# nginx.conf (for production deployment)
server {
    listen 80;
    server_name practicehub.example.com;

    # Redirect HTTP to HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name practicehub.example.com;

    # SSL Configuration
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;

    # Security Headers
    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;

    # Hide Nginx version
    server_tokens off;

    # Rate limiting zones
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/m;
    limit_req_zone $binary_remote_addr zone=auth:10m rate=5r/m;

    # Auth endpoints (stricter limits)
    location /api/auth/ {
        limit_req zone=auth burst=3 nodelay;
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }

    # API endpoints (moderate limits)
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }

    # All other requests
    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }
}
```

**Step 3: Test Security Headers**
```bash
# Install security header checker
pnpm add -D @next/bundle-analyzer

# Start dev server
pnpm dev

# Check headers using curl
curl -I http://localhost:3000 | grep -E "(Strict-Transport|X-Frame|X-Content|CSP)"

# Should see:
# strict-transport-security: max-age=63072000; includeSubDomains; preload
# x-frame-options: SAMEORIGIN
# x-content-type-options: nosniff
# content-security-policy: default-src 'self'; ...
```

**Step 4: Use Online Security Scanner**
```bash
# After deploying to staging/production, test with:
# https://securityheaders.com
# https://observatory.mozilla.org

# Target score: A+ on securityheaders.com
```

**Success Criteria**:
- âœ… All security headers present in response
- âœ… HSTS header configured correctly
- âœ… CSP header allows app to function
- âœ… X-Frame-Options prevents clickjacking
- âœ… App still works normally (no CSP violations)

---

### Task 1.6: Configure Sentry Error Tracking (2-3 hours)

**Priority**: ðŸ”´ CRITICAL (Monitoring)
**Impact**: Production error visibility, debugging
**Files**: `lib/sentry.ts` (new), `app/layout.tsx`, `.env.local`

**Step 1: Create Sentry Account and Project** (10 minutes)
1. Go to https://sentry.io
2. Create account and new project (select Next.js)
3. Copy DSN (Data Source Name)

**Step 2: Install Sentry SDK**
```bash
pnpm add @sentry/nextjs
```

**Step 3: Initialize Sentry**
```bash
# Run Sentry wizard (automated setup)
npx @sentry/wizard@latest -i nextjs

# This creates:
# - sentry.client.config.ts
# - sentry.server.config.ts
# - sentry.edge.config.ts
# - next.config.js updates
```

**Step 4: Configure Environment Variables**
```bash
# .env.local
NEXT_PUBLIC_SENTRY_DSN="https://your-dsn@sentry.io/project-id"
SENTRY_AUTH_TOKEN="your-auth-token"
SENTRY_ORG="your-org"
SENTRY_PROJECT="practice-hub"

# Don't send errors in development
NEXT_PUBLIC_SENTRY_ENVIRONMENT="development"
```

**Step 5: Add Custom Error Tracking**
```typescript
// lib/sentry.ts
import * as Sentry from "@sentry/nextjs";

export function captureError(error: Error, context?: Record<string, any>) {
  Sentry.captureException(error, {
    contexts: {
      custom: context,
    },
  });
}

export function captureMessage(message: string, level: "info" | "warning" | "error" = "info") {
  Sentry.captureMessage(message, level);
}

export function setUserContext(userId: string, email: string, tenantId: string) {
  Sentry.setUser({
    id: userId,
    email,
    tenant: tenantId,
  });
}
```

**Step 6: Add Error Boundaries**
```typescript
// app/error.tsx (if doesn't exist)
"use client";

import { useEffect } from "react";
import * as Sentry from "@sentry/nextjs";
import { Button } from "@/components/ui/button";

export default function Error({
  error,
  reset,
}: {
  error: Error & { digest?: string };
  reset: () => void;
}) {
  useEffect(() => {
    Sentry.captureException(error);
  }, [error]);

  return (
    <div className="flex min-h-screen items-center justify-center">
      <div className="text-center">
        <h2 className="text-2xl font-bold mb-4">Something went wrong!</h2>
        <p className="text-muted-foreground mb-6">
          Our team has been notified and is working on a fix.
        </p>
        <Button onClick={() => reset()}>Try again</Button>
      </div>
    </div>
  );
}
```

**Step 7: Update tRPC Error Handler**
```typescript
// app/server/trpc.ts
import * as Sentry from "@sentry/nextjs";

// Add to onError handler
export const createTRPCContext = async (opts: { headers: Headers }) => {
  const context = await createContext();

  return {
    ...context,
    // Add Sentry transaction
    sentryTransaction: Sentry.startTransaction({
      name: "tRPC Request",
      op: "trpc",
    }),
  };
};

// Error handler
const loggerLink = trpc.loggerLink({
  enabled: (opts) =>
    process.env.NODE_ENV === "development" ||
    (opts.direction === "down" && opts.result instanceof Error),
  onError: (opts) => {
    // Log to Sentry
    Sentry.captureException(opts.error, {
      contexts: {
        trpc: {
          path: opts.path,
          type: opts.type,
          input: opts.input,
        },
      },
    });
  },
});
```

**Testing**:
```bash
# 1. Trigger test error
# Add this to any page:
throw new Error("Test error for Sentry");

# 2. Check Sentry dashboard for error
# Should see error with:
# - Stack trace
# - User context
# - tRPC path (if from tRPC)

# 3. Remove test error
```

**Success Criteria**:
- âœ… Sentry dashboard shows test error
- âœ… Error includes stack trace and context
- âœ… User information captured (when logged in)
- âœ… tRPC errors include procedure path
- âœ… No errors in development console

---

### Task 1.7: Set Up Database Backups (2 hours)

**Priority**: ðŸ”´ CRITICAL (Data Safety)
**Impact**: Disaster recovery, data loss prevention
**Files**: `scripts/backup-db.sh` (new), cron job

**Step 1: Create Backup Script**
```bash
#!/bin/bash
# scripts/backup-db.sh

set -e

# Configuration
BACKUP_DIR="/var/backups/practice-hub"
DB_NAME="practice_hub"
DB_USER="postgres"
DB_PASSWORD="PgHub2024\$Secure#DB!9kL"
DB_HOST="localhost"
DB_PORT="5432"
RETENTION_DAYS=30

# Create backup directory if it doesn't exist
mkdir -p "$BACKUP_DIR"

# Generate timestamp
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_FILE="$BACKUP_DIR/practice_hub_$TIMESTAMP.sql.gz"

# Perform backup
echo "Starting database backup..."
PGPASSWORD="$DB_PASSWORD" pg_dump \
  -h "$DB_HOST" \
  -p "$DB_PORT" \
  -U "$DB_USER" \
  -d "$DB_NAME" \
  --format=plain \
  --no-owner \
  --no-acl \
  | gzip > "$BACKUP_FILE"

# Check if backup was successful
if [ $? -eq 0 ]; then
  echo "Backup completed successfully: $BACKUP_FILE"

  # Get backup size
  SIZE=$(du -h "$BACKUP_FILE" | cut -f1)
  echo "Backup size: $SIZE"
else
  echo "Backup failed!"
  exit 1
fi

# Delete old backups (older than RETENTION_DAYS)
echo "Cleaning up old backups..."
find "$BACKUP_DIR" -name "practice_hub_*.sql.gz" -type f -mtime +$RETENTION_DAYS -delete
echo "Old backups cleaned up (retention: $RETENTION_DAYS days)"

# Optional: Upload to S3 (for production)
if [ -n "$S3_BUCKET" ]; then
  echo "Uploading backup to S3..."
  aws s3 cp "$BACKUP_FILE" "s3://$S3_BUCKET/backups/" --storage-class GLACIER_IR
  echo "Backup uploaded to S3"
fi

echo "Backup process complete!"
```

**Step 2: Make Script Executable**
```bash
chmod +x scripts/backup-db.sh

# Test backup script
./scripts/backup-db.sh
```

**Step 3: Set Up Automated Backups (Cron)**
```bash
# Edit crontab
crontab -e

# Add daily backup at 2 AM
0 2 * * * /root/projects/practice-hub/scripts/backup-db.sh >> /var/log/practice-hub-backup.log 2>&1

# Add weekly verification backup (Sundays at 3 AM)
0 3 * * 0 /root/projects/practice-hub/scripts/verify-backup.sh >> /var/log/practice-hub-backup.log 2>&1
```

**Step 4: Create Restore Script**
```bash
#!/bin/bash
# scripts/restore-db.sh

set -e

if [ -z "$1" ]; then
  echo "Usage: ./scripts/restore-db.sh <backup-file>"
  echo "Example: ./scripts/restore-db.sh /var/backups/practice-hub/practice_hub_20251019_020000.sql.gz"
  exit 1
fi

BACKUP_FILE="$1"
DB_NAME="practice_hub"
DB_USER="postgres"
DB_PASSWORD="PgHub2024\$Secure#DB!9kL"

echo "WARNING: This will delete all data in the '$DB_NAME' database!"
read -p "Are you sure you want to continue? (yes/no): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
  echo "Restore cancelled"
  exit 0
fi

echo "Restoring from: $BACKUP_FILE"

# Drop existing database
PGPASSWORD="$DB_PASSWORD" psql -U "$DB_USER" -c "DROP DATABASE IF EXISTS $DB_NAME;"

# Create fresh database
PGPASSWORD="$DB_PASSWORD" psql -U "$DB_USER" -c "CREATE DATABASE $DB_NAME;"

# Restore backup
gunzip < "$BACKUP_FILE" | PGPASSWORD="$DB_PASSWORD" psql -U "$DB_USER" -d "$DB_NAME"

echo "Database restored successfully!"
```

**Step 5: Test Restore Process**
```bash
# Make restore script executable
chmod +x scripts/restore-db.sh

# Create test backup
./scripts/backup-db.sh

# Test restore (DANGER: This will delete your database!)
# Only do this in development!
./scripts/restore-db.sh /var/backups/practice-hub/practice_hub_YYYYMMDD_HHMMSS.sql.gz
```

**Step 6: Document Backup Procedures**
```markdown
# Add to PRODUCTION_READINESS.md

## Backup Procedures

### Automated Daily Backups
- Run at 2 AM daily via cron
- Stored in `/var/backups/practice-hub/`
- Retention: 30 days
- Format: gzipped SQL dumps

### Manual Backup
bash
./scripts/backup-db.sh


### Restore from Backup
bash
./scripts/restore-db.sh /var/backups/practice-hub/practice_hub_YYYYMMDD_HHMMSS.sql.gz


### Verify Backup Integrity
bash
# Test restore to temporary database
gunzip < backup.sql.gz | psql -U postgres -d test_restore
```

**Success Criteria**:
- âœ… Backup script executes successfully
- âœ… Backup file created and compressed
- âœ… Cron job scheduled
- âœ… Restore script tested (in development)
- âœ… Backup/restore documented

---

## Week 1 Summary

After completing Week 1, you will have:
- âœ… Fixed 2 critical N+1 queries (99% query reduction)
- âœ… Added 5 missing database indexes (50-80% faster queries)
- âœ… Implemented rate limiting (protection against brute force)
- âœ… Added security headers (XSS, clickjacking protection)
- âœ… Configured Sentry monitoring (production error visibility)
- âœ… Set up database backups (disaster recovery)

**Estimated time**: 20-25 hours
**Risk reduction**: CRITICAL â†’ LOW
**Ready for**: Internal testing / staging deployment

---

## Week 2: High Priority Fixes (15-20 hours)

Continue to [Week 2 Tasks](./SECURITY_AUDIT.md#high-priority-fixes) for:
- XSS input sanitization
- 2FA implementation
- Authorization consistency
- Account lockout
- Connection pooling
- Redis caching

---

## Week 3-4: Medium Priority (15-20 hours)

Continue to [Week 3-4 Tasks](./SECURITY_AUDIT.md#medium-priority-fixes) for:
- Row-Level Security
- CSRF protection
- Session timeout
- API authentication
- Monitoring dashboard
- Load testing

---

## Getting Help

If you encounter issues:

1. **Check Documentation**:
   - Performance issues: `PERFORMANCE_AUDIT.md`
   - Security issues: `SECURITY_AUDIT.md`
   - Deployment: `PRODUCTION_READINESS.md`

2. **Review Test Output**:
   ```bash
   pnpm test --reporter=verbose
   ```

3. **Check Logs**:
   ```bash
   # Application logs
   docker compose logs -f app

   # Database logs
   docker compose logs -f db

   # Nginx logs (production)
   tail -f /var/log/nginx/error.log
   ```

4. **Sentry Dashboard**:
   - Check for error patterns
   - Review stack traces
   - Analyze user context

---

**Document Version**: 1.0
**Last Updated**: 2025-10-19
**Next Review**: After Week 1 completion
